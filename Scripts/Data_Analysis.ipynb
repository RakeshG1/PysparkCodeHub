{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Import Libraries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import warnings\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark import SparkContext as sc\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Random Data Generation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "state_list = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"]\n",
    "df = pd.DataFrame({})\n",
    "for i in range(20):\n",
    "    ##### Choose Randomly from List\n",
    "    state = np.random.choice(state_list)\n",
    "    ##### Generate 1 value between 0 and 100\n",
    "    value = np.random.randint(100, size=(1))[0]\n",
    "    new_row = {\"State\":state, \"Value\":value}\n",
    "    # df = df.append(new_row, ignore_index=True)\n",
    "    df = pd.concat([df, pd.DataFrame(new_row, index=[0])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Data Overview`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>D</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>E</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>B</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>C</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>E</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>D</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>F</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>D</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Saving Into .csv File`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_file.name --> /tmp/tmpvxer71h6.csv\n",
      "output_file_path --> /mnt/Local_Host/Git_Repo/PysparkCodeHub/Data/data.csv\n",
      "File saved!!\n"
     ]
    }
   ],
   "source": [
    "# To handle direct file creation issue : Create a temporary file object with automatic cleanup\n",
    "with tempfile.NamedTemporaryFile(suffix=\".csv\", delete=False) as temp_file:\n",
    "    # Save dataframe as temp csv file\n",
    "    df.to_csv(temp_file.name, index=False)\n",
    "    print(\"temp_file.name --> {0}\".format(temp_file.name))\n",
    "    \n",
    "    # Output File Path\n",
    "    output_dir = os.path.dirname(os.getcwd())\n",
    "    output_file_path = output_dir + \"/Data/data.csv\"\n",
    "    print(f\"output_file_path --> {output_file_path}\")\n",
    "\n",
    "    # Copy temp .csv file to actual path\n",
    "    if os.path.exists(output_dir):\n",
    "        # df.to_csv(output_file_path, index=False)\n",
    "        shutil.copy2(temp_file.name, output_file_path)\n",
    "        print(\"File saved!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Creating Spark DataFrame Using Sample Data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- middle_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+----------+-----------+---------+-----+------+-------+\n",
      "|first_name|middle_name|last_name|  dob|gender| salary|\n",
      "+----------+-----------+---------+-----+------+-------+\n",
      "|      AAAA|           |     SSSS|43435|     M|  80000|\n",
      "|     MMMMM|       RRRR|         | 2356|     F|  20000|\n",
      "|      RRRR|           |     WWWW| 2343|      |5460000|\n",
      "|      MMMM|       AAAA|     JJJJ|56734|     F|6570000|\n",
      "|      JJJJ|       MMMM|     BBBB|     |     M|      0|\n",
      "+----------+-----------+---------+-----+------+-------+\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>middle_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>dob</th>\n",
       "      <th>gender</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAA</td>\n",
       "      <td></td>\n",
       "      <td>SSSS</td>\n",
       "      <td>43435</td>\n",
       "      <td>M</td>\n",
       "      <td>80000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MMMMM</td>\n",
       "      <td>RRRR</td>\n",
       "      <td></td>\n",
       "      <td>2356</td>\n",
       "      <td>F</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RRRR</td>\n",
       "      <td></td>\n",
       "      <td>WWWW</td>\n",
       "      <td>2343</td>\n",
       "      <td></td>\n",
       "      <td>5460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MMMM</td>\n",
       "      <td>AAAA</td>\n",
       "      <td>JJJJ</td>\n",
       "      <td>56734</td>\n",
       "      <td>F</td>\n",
       "      <td>6570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JJJJ</td>\n",
       "      <td>MMMM</td>\n",
       "      <td>BBBB</td>\n",
       "      <td></td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spark Session\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName('PySpark Data Analysis')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Sample Data\n",
    "data = [(\"AAAA\",\"\",\"SSSS\",\"43435\",\"M\",80000),\n",
    "        (\"MMMMM\",\"RRRR\",\"\",\"2356\",\"F\",20000),\n",
    "        (\"RRRR\",\"\",\"WWWW\",\"2343\",\"\",5460000),\n",
    "        (\"MMMM\",\"AAAA\",\"JJJJ\",\"56734\",\"F\",6570000),\n",
    "        (\"JJJJ\",\"MMMM\",\"BBBB\",\"\",\"M\",0)]\n",
    "\n",
    "# Data Schema\n",
    "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
    "\n",
    "# Spark Dataframe\n",
    "spark_df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "# Print Data Schema\n",
    "spark_df.printSchema()\n",
    "\n",
    "# Print DataFrame Type\n",
    "print(type(spark_df))\n",
    "\n",
    "# Print Spark Dataframe\n",
    "spark_df.show()  \n",
    "\n",
    "# Convert Spark Dataframe into Pandas Dataframe\n",
    "pandas_df = spark_df.toPandas()\n",
    "print(type(pandas_df))\n",
    "display(HTML(pandas_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Read .CSV File & Register into Spark TempTable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|State|Value|\n",
      "+-----+-----+\n",
      "|    D|   27|\n",
      "|    F|    2|\n",
      "|    D|   67|\n",
      "|    E|   48|\n",
      "|    B|   52|\n",
      "+-----+-----+\n",
      "\n",
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- Value: integer (nullable = true)\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "20\n",
      "+-----+-----+\n",
      "|State|Value|\n",
      "+-----+-----+\n",
      "|    D|   27|\n",
      "|    F|    2|\n",
      "|    D|   67|\n",
      "|    E|   48|\n",
      "|    B|   52|\n",
      "+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)\n",
    "sqlcontext = SQLContext(sc)\n",
    "\n",
    "# Input File Path\n",
    "input_dir = os.path.dirname(os.getcwd())\n",
    "input_file_path = input_dir + \"/Data/data.csv\"\n",
    "\n",
    "# Reads .csv\n",
    "sqlCtx = SQLContext(sc)\n",
    "spark_df_csv_sample_data = sqlCtx\\\n",
    "                .read\\\n",
    "                .format(\"csv\")\\\n",
    "                .options(header='true', inferschema='true', delimiter=',')\\\n",
    "                .load(input_file_path)\n",
    "spark_df_csv_sample_data.registerTempTable(\"spark_df_csv_sample_data\")\n",
    "\n",
    "# Query Over Temp Table\n",
    "sqlCtx.sql(\"select * from spark_df_csv_sample_data limit 5\").show()\n",
    "\n",
    "# Print Data Schema\n",
    "spark_df_csv_sample_data.printSchema()\n",
    "\n",
    "# Print DataFrame Type\n",
    "print(type(spark_df_csv_sample_data))\n",
    "\n",
    "# Count Rows\n",
    "print(spark_df_csv_sample_data.count())\n",
    "\n",
    "# Print Spark Dataframe\n",
    "#spark_df_csv_sample_data.show(5, truncate=30)  \n",
    "#spark_df_csv_sample_data.show(vertical=True)\n",
    "spark_df_csv_sample_data.show(5, truncate=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Exploring Pyspark Predefined Functions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(State='D', Value=27),\n",
       " Row(State='F', Value=2),\n",
       " Row(State='D', Value=67),\n",
       " Row(State='E', Value=48),\n",
       " Row(State='B', Value=52)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# head 5 : way 1\n",
    "spark_df_csv_sample_data.limit(5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(State='D', Value=27), Row(State='F', Value=2)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# head 2 : way 2\n",
    "spark_df_csv_sample_data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|State|\n",
      "+-----+\n",
      "|    D|\n",
      "|    F|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select columns\n",
    "spark_df_csv_sample_data.limit(2).select(\"State\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|State_New|Value|\n",
      "+---------+-----+\n",
      "|        D|   27|\n",
      "|        F|    2|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Column rename\n",
    "spark_df_csv_sample_data\\\n",
    "    .limit(2)\\\n",
    "    .select(col(\"State\").alias(\"State_New\"), col(\"Value\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['State', 'Value']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List preparation based on spark dataframe columns\n",
    "[col for col in spark_df_csv_sample_data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|State|Value|\n",
      "+-----+-----+\n",
      "|    D|   27|\n",
      "|    F|    2|\n",
      "|    D|   67|\n",
      "+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter pyspark columns list\n",
    "spark_df_csv_sample_data.select(spark_df_csv_sample_data.columns[:2]).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|State|Value|\n",
      "+-----+-----+\n",
      "|D    |27   |\n",
      "|F    |2    |\n",
      "|D    |67   |\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all columns\n",
    "spark_df_csv_sample_data.limit(3).select(\"*\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[253] at RDD at PythonRDD.scala:53\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "DataFrame[State: string, Value: bigint]\n",
      "+-----+-----+\n",
      "|State|Value|\n",
      "+-----+-----+\n",
      "|    D|   27|\n",
      "|    F|    2|\n",
      "|    D|   67|\n",
      "|    E|   48|\n",
      "|    B|   52|\n",
      "+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# map() is only be performed on rdd\n",
    "# so converting the dataframe into rdd using df.rdd\n",
    "rdd = spark_df_csv_sample_data\\\n",
    "      .rdd\\\n",
    "      .map(lambda loop: (loop[\"State\"],loop[\"Value\"]))\n",
    "print(rdd)\n",
    "\n",
    "df2 = rdd.toDF([\"State\",\"Value\"])\n",
    "print(type(df2))\n",
    "print(df2)\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('D', 27), ('F', 2)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hHead 2\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D', 'F']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List preparation with columns\n",
    "[x[0] for x in rdd.take(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 2]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List preparation with values\n",
    "[x[1] for x in rdd.take(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('D', 27),\n",
       " ('F', 2),\n",
       " ('D', 67),\n",
       " ('E', 48),\n",
       " ('B', 52),\n",
       " ('F', 13),\n",
       " ('B', 34),\n",
       " ('D', 74),\n",
       " ('A', 76),\n",
       " ('F', 82),\n",
       " ('B', 35),\n",
       " ('E', 35),\n",
       " ('B', 30),\n",
       " ('A', 18),\n",
       " ('C', 80),\n",
       " ('E', 6),\n",
       " ('D', 73),\n",
       " ('F', 30),\n",
       " ('A', 32),\n",
       " ('D', 59)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data collect\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "2\n",
      "67\n",
      "48\n",
      "52\n",
      "13\n",
      "34\n",
      "74\n",
      "76\n",
      "82\n",
      "35\n",
      "35\n",
      "30\n",
      "18\n",
      "80\n",
      "6\n",
      "73\n",
      "30\n",
      "32\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "# Data in the variable \n",
    "table = [x[\"Value\"] for x in spark_df_csv_sample_data.rdd.collect()]\n",
    "\n",
    "# Looping the list for printing \n",
    "for row in table:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "DataFrame[State: string, Value: int]\n",
      "D --> 27\n",
      "F --> 2\n"
     ]
    }
   ],
   "source": [
    "# Looping through rows\n",
    "rows_looped = spark_df_csv_sample_data.select(\"State\", \"Value\").limit(2).collect()\n",
    "print(type(spark_df_csv_sample_data))\n",
    "print(spark_df_csv_sample_data)\n",
    "for rows in rows_looped:\n",
    "    print(rows[0], \"-->\", rows[1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
